{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Linear Regression**\n",
    "###### **In this tutorial we will deal with a regression problem and we will learn how to load the data, analyze the data and apply some pre-processing and apply a linear regression model(having only one layer). And further this tutorial we will extend this to a deep neural network. This will help us deep understanding of keras dense layer and activation functions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Let's Start**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Silence Warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Import Libraries\n",
    "import pandas as pd     # used to work with datasets, analyze and modify them\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# Make numpy printouts easier to read\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Std_Id</th>\n",
       "      <th>Attendance</th>\n",
       "      <th>Class_Test</th>\n",
       "      <th>Mid</th>\n",
       "      <th>Final</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR01</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR02</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>Re_Admit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR03</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>Re_Admit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR04</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>42</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR05</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LR06</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LR07</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LR08</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LR09</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LR10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Std_Id  Attendance  Class_Test  Mid  Final      Type\n",
       "0   LR01           8           8   20     40   Regular\n",
       "1   LR02           9           7   19     32  Re_Admit\n",
       "2   LR03           9           9   26     27  Re_Admit\n",
       "3   LR04           5           8   27     42   Regular\n",
       "4   LR05           7           5   24     37   Regular\n",
       "5   LR06           8           5   19     38   Regular\n",
       "6   LR07           8           5   20     40   Regular\n",
       "7   LR08           5           9   19     33   Regular\n",
       "8   LR09           7          10   27     30   Regular\n",
       "9   LR10           7           8   22     39   Regular"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data.csv')\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Std_Id  Attendance  Class_Test  Mid  Final  Regular  Re_Admit\n",
      "0    LR01           8           8   23     40        1         0\n",
      "1    LR02           9           7   19     32        0         1\n",
      "2    LR03           9           9   26     27        0         1\n",
      "3    LR04           5           8   27     42        1         0\n",
      "4    LR05           7           5   24     37        1         0\n",
      "..    ...         ...         ...  ...    ...      ...       ...\n",
      "95   LR96           8           5   19     40        1         0\n",
      "96   LR97           8           5   20     40        1         0\n",
      "97   LR98           5           9   19     33        1         0\n",
      "98   LR99           7          10   17     30        1         0\n",
      "99  LR100           7           8   22     39        1         0\n",
      "\n",
      "[100 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop the data with missing values. In our dataset there is no missing value, so we need not drop anything.\n",
    "#dataset = dataset.dropna()\n",
    "#dataset\n",
    "\n",
    "# Convert Categorical 'Type' data into one-hot data: In our dataset, all the data are numerical except Type data. So, to ignore confusion of our model we will one-hot encoding the type.\n",
    "dataset = pd.read_csv('data.csv')\n",
    "type = dataset.pop('Type')\n",
    "dataset['Regular'] = (type == 'Regular')*1\n",
    "dataset['Re_Admit'] = (type == 'Re_Admit')*1\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Data into Train-Test**\n",
    "###### **We can do it by calling *sample* that includes what percentage we want to use for training. Remaining percentage automatically goes for testing, We just have to drop the training specified dataset.**\n",
    "###### **Before spliting data we have to remove the *Std_Id* column, because is a string type object and it can confuse the spliting process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Attendance  Class_Test  Mid  Final  Regular  Re_Admit\n",
      "0            8           8   23     40        1         0\n",
      "1            9           7   19     32        0         1\n",
      "2            9           9   26     27        0         1\n",
      "3            5           8   27     42        1         0\n",
      "4            7           5   24     37        1         0\n",
      "..         ...         ...  ...    ...      ...       ...\n",
      "95           8           5   19     40        1         0\n",
      "96           8           5   20     40        1         0\n",
      "97           5           9   19     33        1         0\n",
      "98           7          10   17     30        1         0\n",
      "99           7           8   22     39        1         0\n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Removing the \"Sd_Id\" column\n",
    "id = dataset.pop('Std_Id')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 6) (80, 6) (20, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Attendance</th>\n",
       "      <td>80.0</td>\n",
       "      <td>7.2000</td>\n",
       "      <td>1.335020</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_Test</th>\n",
       "      <td>80.0</td>\n",
       "      <td>7.4625</td>\n",
       "      <td>1.749819</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mid</th>\n",
       "      <td>80.0</td>\n",
       "      <td>21.8375</td>\n",
       "      <td>3.591283</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.25</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final</th>\n",
       "      <td>80.0</td>\n",
       "      <td>35.8250</td>\n",
       "      <td>4.716722</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>40.00</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regular</th>\n",
       "      <td>80.0</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.265053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Re_Admit</th>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.265053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count     mean       std   min   25%   50%    75%   max\n",
       "Attendance   80.0   7.2000  1.335020   5.0   7.0   7.0   8.00  10.0\n",
       "Class_Test   80.0   7.4625  1.749819   5.0   5.0   8.0   9.00  10.0\n",
       "Mid          80.0  21.8375  3.591283  13.0  19.0  21.0  25.25  29.0\n",
       "Final        80.0  35.8250  4.716722  27.0  32.0  37.0  40.00  44.0\n",
       "Regular      80.0   0.9250  0.265053   0.0   1.0   1.0   1.00   1.0\n",
       "Re_Admit     80.0   0.0750  0.265053   0.0   0.0   0.0   0.00   1.0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "print(dataset.shape, train_dataset.shape, test_dataset.shape)\n",
    "train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **In our dataset, we have 10 entries and 7 different columns. From which 8 entries will be used for training and remaining 2 entries will be used for testing. And the dercribe function gives us some parameters that might be used to analyze the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Features**\n",
    "###### **Here, we will split the features from the labels. From our dataset, we will predict *Final* marks so we copy the training and testing dataset as training and testing features and because Final is the label of both training and testing features we pop the *Final* label.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('Final')\n",
    "test_labels = test_features.pop('Final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Features**\n",
    "###### **We will define a simple plot function and can use the function to plot any feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(feature, x=None, y=None):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.scatter(train_features[feature], train_labels, label='Data')\n",
    "    if(x is not None and y is not None):\n",
    "        plt.plot(x, y, color='k', label='Prediction')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Final')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot('Mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize**\n",
    "###### **If we see the description of the data, seen above in Split Data in Train-Test, we can see that the mean values have different ranges. If we take then as it is, it may confuge our model. The best way to overcome it is to normalize the data. For this, we can use a normalization layer from tensorflow preprocessing module.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               mean       std\n",
      "Attendance   7.2000  1.335020\n",
      "Class_Test   7.4625  1.749819\n",
      "Mid         21.8375  3.591283\n",
      "Final       35.8250  4.716722\n",
      "Regular      0.9250  0.265053\n",
      "Re_Admit     0.0750  0.265053\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x0000015E0FADD4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[ 7.2    7.463 21.838  0.925  0.075]]\n"
     ]
    }
   ],
   "source": [
    "# First, see the data\n",
    "print(train_dataset.describe().transpose()[['mean', 'std']])\n",
    "\n",
    "# Normalization Layer\n",
    "normalizer = preprocessing.Normalization()  # This is a keras layer for sequential api.\n",
    "# Adapt to Data: To call the normalizer we have to adapt it to our data.\n",
    "# Because our dataset is a pandas dataset, we need to make it to a numpy array.\n",
    "normalizer.adapt(np.array(train_features))\n",
    "# Now, if we see the normalization value we see that it is exactly the mean value. Because we don't apply the normalization layer so far.\n",
    "print(normalizer.mean.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Let's check out the first training features to be normalized. When the normalize layer is called, it returns the input data with each feature independently normalized. It uses the formula- ((input-mean)/stddev).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Column: [[ 5  9 19  1  0]]\n",
      "Normalized: [[-1.658  0.884 -0.795  0.285 -0.285]]\n"
     ]
    }
   ],
   "source": [
    "first = np.array(train_features[:1])\n",
    "# We will use the normalizer and casting the tensor into numpy array\n",
    "normalized_first = normalizer(first).numpy()\n",
    "print(\"First Column:\", first)\n",
    "print(\"Normalized:\", normalized_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**\n",
    "###### **Now, we will use Linear Regression to predict final ccording to attendance/class_test/mid. For this, we have to normalize the input(attendance/class_test/mid). To predict the output(final) it will use a linear transformation function(y=mx+c) used in dense layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80,) (80, 5)\n"
     ]
    }
   ],
   "source": [
    "# Work with Single Feature: [Not working due to shape issue]\n",
    "# Use \"Attendance\" as input\n",
    "feature = \"Attendance\"\n",
    "single_feature = np.array(train_features[feature])\n",
    "print(single_feature.shape, train_features.shape)\n",
    "# Now, we have to normalize and adapt the frature\n",
    "# Normalization Layer\n",
    "single_feature_normalizer = preprocessing.Normalization()\n",
    "# Adapt to the data\n",
    "#single_feature_normalizer.adapt(single_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Model\n",
    "feature_model = keras.models.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(units=1)   # Linear Model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, 5)                11        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17\n",
      "Trainable params: 6\n",
      "Non-trainable params: 11\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(feature_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "loss = keras.losses.MeanSquaredError()     # |y_p - y|. We can also use MeanSquareError: (y_p - p)^2\n",
    "optim = keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "feature_model.compile(optimizer=optim, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 1s 167ms/step - loss: 1314.9717 - val_loss: 1247.3296\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1298.5729 - val_loss: 1239.3755\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1281.5972 - val_loss: 1230.7065\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 1265.8490 - val_loss: 1222.3663\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1249.7800 - val_loss: 1212.3485\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1235.6140 - val_loss: 1203.6200\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1220.1604 - val_loss: 1194.4014\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 1204.9908 - val_loss: 1185.3727\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1191.5731 - val_loss: 1176.8938\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1176.5286 - val_loss: 1167.5481\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 1162.0851 - val_loss: 1158.2563\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 1147.9648 - val_loss: 1149.1925\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 1134.1992 - val_loss: 1139.6681\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 1120.5005 - val_loss: 1130.0874\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 1106.7638 - val_loss: 1120.6874\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1093.3469 - val_loss: 1111.4840\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1080.1975 - val_loss: 1102.3601\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1066.9363 - val_loss: 1092.9900\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1054.8546 - val_loss: 1083.7194\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1041.8528 - val_loss: 1074.7314\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1028.7931 - val_loss: 1065.1003\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 1016.3735 - val_loss: 1054.6658\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1004.6042 - val_loss: 1044.5432\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 992.2462 - val_loss: 1035.6812\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 979.9926 - val_loss: 1025.7853\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 968.1890 - val_loss: 1016.4293\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 956.6429 - val_loss: 1007.3278\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 944.9291 - val_loss: 997.9550\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 933.2186 - val_loss: 987.8457\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 921.7563 - val_loss: 978.0089\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 910.5657 - val_loss: 968.3864\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 899.5960 - val_loss: 958.6334\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 888.6122 - val_loss: 949.1540\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 877.4274 - val_loss: 939.1268\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 866.7050 - val_loss: 928.9637\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 855.9929 - val_loss: 918.4840\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 845.3641 - val_loss: 908.5912\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 835.2397 - val_loss: 899.2128\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 824.8526 - val_loss: 888.8694\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 814.3632 - val_loss: 878.6689\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 804.3270 - val_loss: 868.7872\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 794.2098 - val_loss: 858.8378\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 852.887 - 0s 42ms/step - loss: 784.7458 - val_loss: 847.9222\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 774.8290 - val_loss: 837.6495\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 765.1202 - val_loss: 827.2502\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 755.2784 - val_loss: 817.8098\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 745.7059 - val_loss: 808.2867\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 736.2009 - val_loss: 799.0361\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 726.9070 - val_loss: 790.1034\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 717.7523 - val_loss: 780.9436\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 708.6027 - val_loss: 771.6541\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 699.6974 - val_loss: 762.1767\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 690.5894 - val_loss: 753.3926\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 681.7833 - val_loss: 744.0648\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 672.9922 - val_loss: 735.2715\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 664.3789 - val_loss: 726.0309\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 655.7783 - val_loss: 716.7085\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 647.4603 - val_loss: 708.1259\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 638.9536 - val_loss: 698.9008\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 630.6221 - val_loss: 689.8608\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 622.3467 - val_loss: 681.4174\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 614.1478 - val_loss: 672.5951\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 606.1470 - val_loss: 663.9258\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 598.2098 - val_loss: 655.1335\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 590.3165 - val_loss: 646.6814\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 582.4250 - val_loss: 638.2494\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 574.7321 - val_loss: 630.0919\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 567.1741 - val_loss: 622.1348\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 559.6104 - val_loss: 613.7266\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 552.2354 - val_loss: 605.0543\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 544.7666 - val_loss: 597.0728\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 537.5804 - val_loss: 588.9274\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 530.3063 - val_loss: 580.9209\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 523.4023 - val_loss: 572.7771\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 515.9923 - val_loss: 564.8953\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 508.9925 - val_loss: 557.4016\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 502.0612 - val_loss: 549.9720\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 495.2019 - val_loss: 542.3647\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 488.6326 - val_loss: 534.7709\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 481.8163 - val_loss: 527.3045\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 475.1585 - val_loss: 520.0168\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 468.7816 - val_loss: 512.3915\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 462.3779 - val_loss: 505.5235\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 455.9455 - val_loss: 498.3691\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 449.6784 - val_loss: 490.9368\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 443.3007 - val_loss: 484.0009\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 437.0706 - val_loss: 476.9900\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 431.0944 - val_loss: 469.8997\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 424.9698 - val_loss: 463.3531\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 419.0460 - val_loss: 456.6060\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 413.1327 - val_loss: 449.9356\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 407.4849 - val_loss: 442.9022\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 401.6140 - val_loss: 436.3006\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 395.8348 - val_loss: 430.1098\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 390.1505 - val_loss: 423.8561\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 384.6331 - val_loss: 417.7059\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 379.2339 - val_loss: 411.5608\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 373.8147 - val_loss: 405.7952\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 368.5717 - val_loss: 399.4779\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 363.3007 - val_loss: 393.2362\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 358.0207 - val_loss: 387.5787\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 352.8515 - val_loss: 381.9102\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 347.6540 - val_loss: 376.2509\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 342.7247 - val_loss: 370.7668\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 337.6582 - val_loss: 365.3600\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 332.9465 - val_loss: 359.6338\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 327.9745 - val_loss: 354.2493\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 323.2011 - val_loss: 348.6879\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 318.4660 - val_loss: 343.2484\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 313.7240 - val_loss: 338.0125\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 309.2039 - val_loss: 332.9335\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 304.5936 - val_loss: 327.8592\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 300.0900 - val_loss: 322.9375\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 295.6262 - val_loss: 318.0465\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 291.3508 - val_loss: 313.0748\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 287.0789 - val_loss: 308.1003\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 282.7402 - val_loss: 303.2153\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 278.5585 - val_loss: 298.3269\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 274.2762 - val_loss: 293.7563\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 270.2646 - val_loss: 289.3391\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 266.1188 - val_loss: 284.8582\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 262.2382 - val_loss: 280.4391\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 258.2474 - val_loss: 276.0573\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 254.3182 - val_loss: 271.6847\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 250.5369 - val_loss: 267.3362\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 246.7703 - val_loss: 263.3224\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 242.9983 - val_loss: 259.2118\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 239.4101 - val_loss: 254.8146\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 235.6885 - val_loss: 250.6504\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 232.0095 - val_loss: 246.7681\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 228.5286 - val_loss: 242.8553\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 225.1174 - val_loss: 239.1643\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 221.6568 - val_loss: 235.2111\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 218.2047 - val_loss: 231.4647\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 214.9352 - val_loss: 227.6141\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 211.4673 - val_loss: 223.9839\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 208.3126 - val_loss: 220.3482\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 205.0386 - val_loss: 216.8547\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 201.9147 - val_loss: 213.5042\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 177.836 - 0s 40ms/step - loss: 198.7932 - val_loss: 210.2170\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 195.7606 - val_loss: 206.7496\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 192.8558 - val_loss: 203.2407\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 189.7077 - val_loss: 199.9680\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 186.7818 - val_loss: 196.7312\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 183.9977 - val_loss: 193.3279\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 180.9640 - val_loss: 190.3192\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 178.1892 - val_loss: 187.1559\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 175.5249 - val_loss: 183.8636\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 172.6571 - val_loss: 180.8403\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 169.9963 - val_loss: 177.7713\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 167.3233 - val_loss: 174.9609\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 164.7106 - val_loss: 172.0394\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 162.0658 - val_loss: 169.2379\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 159.5801 - val_loss: 166.5669\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 157.1157 - val_loss: 163.9405\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 154.6796 - val_loss: 161.0520\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 152.1782 - val_loss: 158.3894\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 149.7854 - val_loss: 155.7852\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 147.4420 - val_loss: 153.1622\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 145.2057 - val_loss: 150.4256\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 142.7858 - val_loss: 147.9453\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 140.5243 - val_loss: 145.4968\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 138.3689 - val_loss: 142.9810\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 136.2060 - val_loss: 140.5876\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 133.9729 - val_loss: 138.3163\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 131.8736 - val_loss: 136.1234\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 129.8170 - val_loss: 133.7413\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 127.7712 - val_loss: 131.4437\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 125.7302 - val_loss: 129.1643\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 123.7476 - val_loss: 127.0154\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 121.7742 - val_loss: 124.9130\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 119.9335 - val_loss: 122.7621\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 118.0182 - val_loss: 120.7604\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 116.0855 - val_loss: 118.7565\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 114.3703 - val_loss: 116.7780\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 112.4581 - val_loss: 114.7866\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 110.6482 - val_loss: 112.9068\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 108.9703 - val_loss: 111.0968\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 107.1877 - val_loss: 109.2068\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 105.6170 - val_loss: 107.2391\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 103.9756 - val_loss: 105.5095\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 102.2203 - val_loss: 103.7589\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 100.6494 - val_loss: 101.9937\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 99.0651 - val_loss: 100.3777\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 97.5583 - val_loss: 98.5468\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 95.9625 - val_loss: 96.9087\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 94.4454 - val_loss: 95.2636\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 93.0168 - val_loss: 93.7419\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 91.5560 - val_loss: 92.2548\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 90.1176 - val_loss: 90.7192\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 88.6987 - val_loss: 89.1526\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 90.77 - 0s 33ms/step - loss: 87.3576 - val_loss: 87.6012\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 85.9442 - val_loss: 86.1134\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 84.5800 - val_loss: 84.6877\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 83.3590 - val_loss: 83.3483\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 82.0858 - val_loss: 81.9145\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 80.7483 - val_loss: 80.5431\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 79.4681 - val_loss: 79.2282\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 78.3112 - val_loss: 77.8664\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 77.0831 - val_loss: 76.6178\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 75.8763 - val_loss: 75.3833\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 74.8050 - val_loss: 74.0630\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 73.5945 - val_loss: 72.8886\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 72.5182 - val_loss: 71.7820\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 71.3873 - val_loss: 70.6324\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 70.2871 - val_loss: 69.5109\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 69.2371 - val_loss: 68.4686\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 68.2561 - val_loss: 67.3008\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 67.1661 - val_loss: 66.2577\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 66.1385 - val_loss: 65.2039\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 65.1754 - val_loss: 64.1612\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 64.2068 - val_loss: 63.1445\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 63.3041 - val_loss: 62.1903\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 62.3299 - val_loss: 61.1954\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 61.4190 - val_loss: 60.1976\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60.4977 - val_loss: 59.3010\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 59.6158 - val_loss: 58.3737\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 58.7276 - val_loss: 57.4459\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 57.9550 - val_loss: 56.5686\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 57.0979 - val_loss: 55.6488\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 56.2725 - val_loss: 54.7871\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 55.4487 - val_loss: 53.9695\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 54.6980 - val_loss: 53.1179\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 53.9199 - val_loss: 52.2686\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 53.1242 - val_loss: 51.5137\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 52.3853 - val_loss: 50.7856\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 51.6833 - val_loss: 50.0526\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 50.9365 - val_loss: 49.3005\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 50.2622 - val_loss: 48.5666\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 49.5708 - val_loss: 47.9064\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 48.8681 - val_loss: 47.2460\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 48.1982 - val_loss: 46.5846\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 47.5822 - val_loss: 45.9354\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 50.47 - 0s 36ms/step - loss: 46.9349 - val_loss: 45.2465\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 46.3197 - val_loss: 44.6357\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 45.6604 - val_loss: 44.0633\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 45.0857 - val_loss: 43.4431\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 44.5348 - val_loss: 42.8537\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 43.9334 - val_loss: 42.3057\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 45.63 - 0s 33ms/step - loss: 43.3881 - val_loss: 41.7642\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 42.8015 - val_loss: 41.1979\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 42.2824 - val_loss: 40.5716\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 41.7235 - val_loss: 40.0414\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 41.1848 - val_loss: 39.5102\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 40.7280 - val_loss: 38.9818\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 40.2191 - val_loss: 38.4517\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 39.7268 - val_loss: 37.9575\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 39.2354 - val_loss: 37.4800\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 38.7700 - val_loss: 37.0136\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 38.2928 - val_loss: 36.5715\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 37.8390 - val_loss: 36.1288\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 37.3973 - val_loss: 35.7150\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 36.9715 - val_loss: 35.2991\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 36.5297 - val_loss: 34.8786\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 28.00 - 0s 40ms/step - loss: 36.1120 - val_loss: 34.5096\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 35.7441 - val_loss: 34.0791\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 35.3079 - val_loss: 33.7258\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 34.9347 - val_loss: 33.3527\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 34.5593 - val_loss: 33.0034\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 34.1570 - val_loss: 32.6714\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 33.8216 - val_loss: 32.3256\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 33.4769 - val_loss: 31.9517\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 33.1133 - val_loss: 31.6171\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 32.7733 - val_loss: 31.2859\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 32.4323 - val_loss: 30.9418\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 32.1018 - val_loss: 30.6071\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 31.7786 - val_loss: 30.2724\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 31.4519 - val_loss: 30.0168\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 31.1632 - val_loss: 29.7086\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 30.8693 - val_loss: 29.4272\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 30.5686 - val_loss: 29.1605\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 30.2661 - val_loss: 28.8743\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 30.0022 - val_loss: 28.6645\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 29.7273 - val_loss: 28.4255\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 29.4622 - val_loss: 28.2033\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 29.2109 - val_loss: 27.9385\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 28.9465 - val_loss: 27.6948\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 28.6755 - val_loss: 27.4722\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 28.4270 - val_loss: 27.2189\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 28.1769 - val_loss: 27.0269\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 27.9531 - val_loss: 26.8446\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 27.7183 - val_loss: 26.6255\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 27.4902 - val_loss: 26.4468\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 27.2764 - val_loss: 26.2635\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 27.0650 - val_loss: 26.0575\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 26.8435 - val_loss: 25.8673\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 26.6491 - val_loss: 25.6937\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 26.4636 - val_loss: 25.5188\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 26.2543 - val_loss: 25.3348\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 26.0738 - val_loss: 25.1706\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 25.8727 - val_loss: 25.0102\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 25.6988 - val_loss: 24.8352\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 25.5069 - val_loss: 24.6505\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 25.3510 - val_loss: 24.4821\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 25.1787 - val_loss: 24.2972\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 25.0269 - val_loss: 24.1836\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 24.8696 - val_loss: 24.0571\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 24.6957 - val_loss: 23.9235\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 24.5271 - val_loss: 23.7991\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 24.3837 - val_loss: 23.6835\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 23.50 - 0s 38ms/step - loss: 24.2244 - val_loss: 23.5801\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 24.0883 - val_loss: 23.4680\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 23.9573 - val_loss: 23.3682\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 23.8159 - val_loss: 23.2704\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 23.6895 - val_loss: 23.1867\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 23.5484 - val_loss: 23.0614\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 23.4186 - val_loss: 22.9665\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 23.2951 - val_loss: 22.8646\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 23.1946 - val_loss: 22.7594\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 23.0532 - val_loss: 22.6660\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 22.9514 - val_loss: 22.5584\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 22.8397 - val_loss: 22.4765\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 22.7240 - val_loss: 22.3962\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 22.6093 - val_loss: 22.3288\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 22.5149 - val_loss: 22.2206\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 22.4131 - val_loss: 22.1331\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 22.2966 - val_loss: 22.0627\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 22.2004 - val_loss: 22.0006\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 22.1226 - val_loss: 21.9386\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 22.0281 - val_loss: 21.8666\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 21.9333 - val_loss: 21.7962\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 21.8517 - val_loss: 21.7432\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 21.7680 - val_loss: 21.6789\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 21.6679 - val_loss: 21.6259\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 21.5952 - val_loss: 21.5677\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 21.5101 - val_loss: 21.5117\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 21.4379 - val_loss: 21.4748\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 21.3589 - val_loss: 21.4212\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 21.2985 - val_loss: 21.3646\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 21.2288 - val_loss: 21.3142\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 21.1594 - val_loss: 21.2745\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 21.0889 - val_loss: 21.2227\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 21.0170 - val_loss: 21.1973\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 20.9603 - val_loss: 21.1595\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 20.8909 - val_loss: 21.1165\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 20.8371 - val_loss: 21.0923\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 20.7631 - val_loss: 21.0376\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 20.7116 - val_loss: 20.9801\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 20.6514 - val_loss: 20.9700\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 20.6056 - val_loss: 20.9277\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 20.5485 - val_loss: 20.9059\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 20.5013 - val_loss: 20.8848\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 20.4398 - val_loss: 20.8719\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 20.3949 - val_loss: 20.8384\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 20.3515 - val_loss: 20.8348\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 20.3027 - val_loss: 20.8037\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 20.2619 - val_loss: 20.7517\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 20.2116 - val_loss: 20.7077\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 20.1771 - val_loss: 20.6864\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 20.1231 - val_loss: 20.6743\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 20.0783 - val_loss: 20.6692\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 20.0569 - val_loss: 20.6828\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 20.0109 - val_loss: 20.6816\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 19.9740 - val_loss: 20.6702\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 19.9303 - val_loss: 20.6617\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 19.9103 - val_loss: 20.6385\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 19.8622 - val_loss: 20.6267\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 19.8278 - val_loss: 20.6175\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 19.7992 - val_loss: 20.6070\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 19.7727 - val_loss: 20.5891\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 19.7408 - val_loss: 20.5565\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 19.7017 - val_loss: 20.5584\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 19.6640 - val_loss: 20.5522\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 19.6447 - val_loss: 20.5121\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 19.6131 - val_loss: 20.4976\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 19.5999 - val_loss: 20.4721\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 19.5583 - val_loss: 20.4512\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 19.5344 - val_loss: 20.4529\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 19.5312 - val_loss: 20.4390\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 19.4929 - val_loss: 20.4252\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 19.4652 - val_loss: 20.4153\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 19.4476 - val_loss: 20.4208\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 19.4207 - val_loss: 20.4368\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 19.4006 - val_loss: 20.4358\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 19.3842 - val_loss: 20.4359\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 19.3547 - val_loss: 20.4473\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 19.3381 - val_loss: 20.4495\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 19.3146 - val_loss: 20.4325\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 19.3017 - val_loss: 20.4364\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 19.2827 - val_loss: 20.4309\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 19.2687 - val_loss: 20.4257\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 19.2433 - val_loss: 20.4264\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 19.2288 - val_loss: 20.4326\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 19.2260 - val_loss: 20.4019\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 19.1966 - val_loss: 20.4052\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 19.1925 - val_loss: 20.3943\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 19.1727 - val_loss: 20.3747\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 19.1644 - val_loss: 20.3772\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 19.1510 - val_loss: 20.4015\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 19.1331 - val_loss: 20.3999\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 19.1139 - val_loss: 20.3991\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 19.1012 - val_loss: 20.4027\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 19.0929 - val_loss: 20.4175\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 19.0777 - val_loss: 20.4203\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 19.0804 - val_loss: 20.4325\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 19.0689 - val_loss: 20.4526\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 19.0504 - val_loss: 20.4341\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 19.0355 - val_loss: 20.4417\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 19.0341 - val_loss: 20.4439\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 19.0177 - val_loss: 20.4594\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 19.0080 - val_loss: 20.4669\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 18.9945 - val_loss: 20.4769\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 18.9914 - val_loss: 20.4617\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 18.9753 - val_loss: 20.4698\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 18.9769 - val_loss: 20.4572\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 18.9618 - val_loss: 20.4508\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 18.9570 - val_loss: 20.4485\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 18.9528 - val_loss: 20.4611\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 18.9406 - val_loss: 20.4585\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 18.9295 - val_loss: 20.4806\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 18.9229 - val_loss: 20.4745\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 18.9217 - val_loss: 20.4965\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 18.9165 - val_loss: 20.5112\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 18.9063 - val_loss: 20.4967\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 18.8937 - val_loss: 20.5023\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 18.8951 - val_loss: 20.5298\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 18.8905 - val_loss: 20.5176\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 18.8893 - val_loss: 20.5077\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 18.8869 - val_loss: 20.5264\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 18.8719 - val_loss: 20.5489\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 18.8778 - val_loss: 20.5692\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 18.8632 - val_loss: 20.5654\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 18.8590 - val_loss: 20.5848\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18.8530 - val_loss: 20.5953\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 18.8502 - val_loss: 20.5813\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 18.8402 - val_loss: 20.5786\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 18.8453 - val_loss: 20.5886\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 18.8324 - val_loss: 20.5903\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 18.8324 - val_loss: 20.6044\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 18.8343 - val_loss: 20.5677\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 18.8283 - val_loss: 20.5469\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 18.8254 - val_loss: 20.5588\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 18.8203 - val_loss: 20.5688\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 18.8139 - val_loss: 20.5902\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 18.8087 - val_loss: 20.5902\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 18.8130 - val_loss: 20.5686\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 18.8135 - val_loss: 20.6134\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 18.8009 - val_loss: 20.6060\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 18.8041 - val_loss: 20.5873\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 18.8001 - val_loss: 20.5753\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 18.7916 - val_loss: 20.5780\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 18.7870 - val_loss: 20.5820\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 18.7991 - val_loss: 20.5997\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 18.7877 - val_loss: 20.6334\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 18.7849 - val_loss: 20.6273\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 18.7767 - val_loss: 20.6331\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 18.7784 - val_loss: 20.6239\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 18.7779 - val_loss: 20.6472\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 18.7743 - val_loss: 20.6634\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 18.7739 - val_loss: 20.6910\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 18.7749 - val_loss: 20.6693\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 18.7770 - val_loss: 20.6694\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 18.7626 - val_loss: 20.6659\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 18.7617 - val_loss: 20.6775\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 18.7750 - val_loss: 20.6425\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 18.7640 - val_loss: 20.6617\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 18.7577 - val_loss: 20.6879\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 18.7573 - val_loss: 20.6959\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 18.7589 - val_loss: 20.7100\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 18.7656 - val_loss: 20.7296\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 18.7521 - val_loss: 20.7396\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 18.7572 - val_loss: 20.7655\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 18.7558 - val_loss: 20.7378\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 18.7487 - val_loss: 20.7244\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 18.7492 - val_loss: 20.7184\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 18.7489 - val_loss: 20.7354\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 18.7470 - val_loss: 20.7449\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 18.7545 - val_loss: 20.7405\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 18.7473 - val_loss: 20.7544\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 18.7420 - val_loss: 20.7672\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 18.7437 - val_loss: 20.7837\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 18.7421 - val_loss: 20.7617\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 18.7412 - val_loss: 20.7691\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 18.7392 - val_loss: 20.7705\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 18.7422 - val_loss: 20.7571\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 18.7359 - val_loss: 20.7545\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 18.7407 - val_loss: 20.7495\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 18.7389 - val_loss: 20.7590\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 18.7358 - val_loss: 20.7754\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 18.7360 - val_loss: 20.7777\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 18.7342 - val_loss: 20.7829\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 18.7361 - val_loss: 20.7775\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 18.7442 - val_loss: 20.7961\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 18.7360 - val_loss: 20.7779\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 18.7376 - val_loss: 20.7648\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 18.7328 - val_loss: 20.7822\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 18.7298 - val_loss: 20.7937\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18.7462 - val_loss: 20.7752\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 18.7320 - val_loss: 20.7822\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 18.7287 - val_loss: 20.7896\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 18.7331 - val_loss: 20.7899\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 18.7264 - val_loss: 20.7955\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 18.7276 - val_loss: 20.8147\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 18.7484 - val_loss: 20.8394\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 18.7260 - val_loss: 20.8450\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 18.7324 - val_loss: 20.8823\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 18.7256 - val_loss: 20.8885\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 18.7374 - val_loss: 20.9029\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 18.7342 - val_loss: 20.9187\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 18.7279 - val_loss: 20.8930\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to Data\n",
    "history = feature_model.fit(\n",
    "    train_features, train_labels,\n",
    "    epochs=500,\n",
    "    verbose=1,\n",
    "    # Use 20% of data for validation\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    #plt.ylim([0,25])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error[Final]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step - loss: 24.7753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24.775251388549805"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_model.evaluate(\n",
    "    test_features, test_labels,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "feature = \"Mid\"\n",
    "range_min = np.min(test_features[feature])\n",
    "#print(range_min)\n",
    "range_max = np.max(test_features[feature])\n",
    "#print(range_max)\n",
    "x = tf.linspace([range_min], [range_max], 100)\n",
    "#x = [[23],[29],[19]]\n",
    "y = abs(feature_model.predict(x))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(feature, x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24d7359adec4ffe2916680474ceb48a86338759ffb8252cd67d6683f84078a4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
